{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NkM20/IA/blob/main/LLMs_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What is Hugging Face?   ðŸ¤—\n",
        "Hugging Face is a company and open-source platform that provides tools and resources for Natural Language Processing (NLP), machine learning (ML), and artificial intelligence (AI). It has become one of the most widely used platforms for building, training, and deploying AI models, especially in the realm of transformer-based models like BERT, GPT, and T5."
      ],
      "metadata": {
        "id": "WkvlzvD9QwNw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmrbd9G52ybf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Key Features of Hugging Face\n",
        "1. Transformers Library\n",
        "\n",
        "The Transformers library is Hugging Face's flagship product.\n",
        "It provides access to state-of-the-art pre-trained models for NLP and beyond, including models for:\n",
        "Text classification\n",
        "Question answering\n",
        "Language translation\n",
        "Text generation\n",
        "Summarization\n",
        "Conversational AI (chatbots)\n",
        "Example models:\n",
        "BERT, GPT-2, GPT-3, T5, RoBERTa, DistilBERT, etc.\n",
        "2. Datasets Library\n",
        "\n",
        "A repository of thousands of datasets for training and fine-tuning ML models.\n",
        "Easily load and preprocess datasets for tasks like:\n",
        "Text classification\n",
        "Machine translation\n",
        "Speech processing\n",
        "Image processing\n",
        "3. Hugging Face Hub\n",
        "\n",
        "An online model repository for sharing and accessing pre-trained models.\n",
        "Contains thousands of pre-trained models uploaded by researchers, organizations, and the community.\n",
        "Models can be downloaded and used with a simple API.\n",
        "4. Accelerate Library\n",
        "\n",
        "Provides tools for training large models efficiently on distributed systems and GPUs/TPUs.\n",
        "5. Inference API\n",
        "\n",
        "Hugging Face offers a cloud-based API to deploy and use models in production without worrying about infrastructure.\n",
        "6. Integration with Other Frameworks\n",
        "\n",
        "Hugging Face integrates with popular ML frameworks like:\n",
        "PyTorch\n",
        "TensorFlow\n",
        "ONNX"
      ],
      "metadata": {
        "id": "kQkGxAe9RNUm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgq2Ci9LC-GM"
      },
      "source": [
        "# Installation and Configuration\n",
        "\n",
        ">\n",
        "\n",
        "The libraries below are essential for developing machine learning and natural language processing (NLP) models. *Transformers*, by HuggingFace, offers a wide range of pre-trained models like BERT, GPT, and T5 for NLP tasks. *Einops* makes it easy to manipulate tensors with a clear syntax, making complex operations simpler. *Accelerate*, also from HuggingFace, helps optimize model training on different hardware accelerators like GPUs and TPUs. Finally, *BitsAndBytes* enables efficient quantization of large models, reducing memory consumption in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "087RXeWuEuIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bitsandbytes-cuda110 bitsandbytes"
      ],
      "metadata": {
        "id": "Bb8Nlsa7N_Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our next cell, we will configure the environment by importing the necessary libraries and configuring our device.\n",
        "\n",
        "Let's import some components from the transformers library\n",
        "\n",
        "* AutoModelForCausalLM: A class that provides a pre-trained causal (or autoregressive) language model (e.g. GPT-2, GPT-3) that are suitable for text generation tasks.\n",
        "* AutoTokenizer: A class that provides a tokenizer that matches the model. The tokenizer is responsible for converting text into (numeric) tokens that the model can understand.\n",
        "* pipeline: Provides a simple, unified interface for various NLP tasks, making it easier to perform tasks such as text generation, classification, and translation.\n",
        "* BitsAndBytesConfig: A class for configuring quantization and other low-level optimizations to improve computational efficiency.\n",
        "\n"
      ],
      "metadata": {
        "id": "tTReUoV61NZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "IAOvkmDVFsRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are also defining the device variable, which specifies the computing device to use:\n",
        "\n",
        "* This line checks if a CUDA-enabled GPU is available. If so, the code sets the device to cuda:0 (the first GPU). If it isn't, it goes back to using the CPU.\n",
        "\n",
        "Remembering that the use of GPU can significantly accelerate the training and inference of deep learning models. Let's take advantage of Colab's free GPU (T4)."
      ],
      "metadata": {
        "id": "8fc1x8FwRnYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "SoLiyxGHGJYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "EnUlkSV_GegP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although not necessary, you can also define a seed, to ensure reproducibility between different experiments and runs.\n",
        "\n",
        "This way, we can ensure that the same random numbers are generated every time the code is run, leading to consistent results."
      ],
      "metadata": {
        "id": "KyeVpz3gFVAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(42)"
      ],
      "metadata": {
        "id": "4pgezw8uGlwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vo6_TUJ83LY"
      },
      "source": [
        "## Token definition\n",
        "\n",
        "Below you must paste your generated token into the Hugging Face panel.\n",
        "So, go to the Hugging Face, signup and generate a new READ token to past it here..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()\n",
        "\n"
      ],
      "metadata": {
        "id": "Sl5wzbFwHONX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_778BQsKb4xP"
      },
      "source": [
        "## Loading the Model\n",
        "\n",
        "In this step, we will download and configure a HuggingFace template. This process may take a few minutes as the template is a few GB - but overall the download to Colab should be relatively quick.\n",
        "\n",
        "\n",
        "First let's start by showing the Phi 3 (microsoft/Phi-3-mini-4k-instruct), a smaller model but which proved to be very interesting and comparable to much larger ones.\n",
        "\n",
        "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
        "\n",
        "It was chosen because it is open source, accessible and can respond well in Portuguese (although it is still better in English). You will see that many models do not understand this language, and those that do are too heavy for us to run in our environment, that is, we need to access them via an API or web interface, such as ChatGPT. However, at this moment we want to explore open source solutions, to obtain greater freedom.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_model = \"microsoft/Phi-3-mini-4k-instruct\""
      ],
      "metadata": {
        "id": "0-uMM1GvICgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `device_map=\"cuda\"`: Specifies that the model should be loaded on a CUDA-enabled GPU. Remembering that this is one of the main advantages of now using Colab as GPU significantly improves inference and model training performance by leveraging parallel processing.\n",
        "\n",
        "* `torch_dtype=\"auto\"`: Automatically sets the appropriate data type for the model's tensors. This ensures that the model uses the best data type for performance and memory efficiency, typically float32 or float16.\n",
        "\n",
        "* `trust_remote_code=True`: Allows loading custom code from the template repository into HuggingFace. This is necessary for certain models that require specific configurations or implementations not included in the standard library.\n",
        "\n",
        "* `attn_implementation=\"eager\"`: Specifies the implementation method for the attention mechanism. The \"eager\" configuration is a particular implementation that can provide better performance for some models by processing the attention mechanism in a specific way."
      ],
      "metadata": {
        "id": "V8vwpKiZFoDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    id_model,\n",
        "    device_map = \"cuda\",\n",
        "    torch_dtype = \"auto\",\n",
        "    trust_remote_code = True,\n",
        "    attn_implementation=\"eager\"\n",
        ")"
      ],
      "metadata": {
        "id": "WDC4iSmzI0sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E588tpOHZjm4"
      },
      "source": [
        "##Tokenizer\n",
        "\n",
        "In our configuration, we also need to load the tokenizer associated with the model. The tokenizer is crucial for preparing text data in a format that the model can understand.\n",
        "\n",
        "* A tokenizer converts raw text into tokens, which are numeric representations that the model can process. It also converts the model's output tokens back into human-readable text.\n",
        "* Tokenizers handle tasks such as splitting text into words or subwords, adding special tokens, and managing vocabulary mapping.\n",
        "\n",
        "\n",
        "\n",
        "The tokenizer is a crucial component in the NLP pipeline, bridging the gap between raw text and model-ready tokens.\n",
        "\n",
        "To implement, we will use the `AutoTokenizer.from_pretrained()` function, specifying the same tokenizer as the model, thus ensuring consistency between text processing during training and inference."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(id_model)"
      ],
      "metadata": {
        "id": "t9xzPcHJJ9Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZJNvKdRaqPK"
      },
      "source": [
        "## Pipeline Creation\n",
        "\n",
        "Now we will create a pipeline for text generation using our previously loaded model and tokenizer. The HuggingFace pipeline function simplifies the process of performing various natural language processing tasks by providing a high-level interface.\n",
        "\n",
        "A pipeline is an abstraction that simplifies the use of pre-trained models for a variety of NLP tasks. It provides a unified API for different tasks like text generation, text classification, translation, and more.\n",
        "\n",
        "\n",
        "Parameters:\n",
        "\n",
        "* `\"text-generation\"`: specifies the task that the pipeline is configured to execute. In this case, we are setting up a pipeline for text generation. The pipeline will use the template to generate text based on a provided prompt.\n",
        "* `model=model`: specifies the pre-trained model that the pipeline will use. Here, we are passing the model that we loaded earlier. This model is responsible for generating text based on input tokens.\n",
        "* `tokenizer=tokenizer`: specifies the tokenizer that the pipeline will use. We pass the tokenizer we loaded earlier to ensure that the input text is tokenized correctly and the output tokens are decoded accurately."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "tng_Nrz7KIsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1jqDqnVay2T"
      },
      "source": [
        "## Pipeline Creation\n",
        "\n",
        "Now we will create a pipeline for text generation using our previously loaded model and tokenizer. The HuggingFace pipeline function simplifies the process of performing various natural language processing tasks by providing a high-level interface.\n",
        "\n",
        "A pipeline is an abstraction that simplifies the use of pre-trained models for a variety of NLP tasks. It provides a unified API for different tasks like text generation, text classification, translation, and more.\n",
        "\n",
        "\n",
        "Parameters:\n",
        "\n",
        "* `\"text-generation\"`: specifies the task that the pipeline is configured to execute. In this case, we are setting up a pipeline for text generation. The pipeline will use the template to generate text based on a provided prompt.\n",
        "* `model=model`: specifies the pre-trained model that the pipeline will use. Here, we are passing the model that we loaded earlier. This model is responsible for generating text based on input tokens.\n",
        "* `tokenizer=tokenizer`: specifies the tokenizer that the pipeline will use. We pass the tokenizer we loaded earlier to ensure that the input text is tokenized correctly and the output tokens are decoded accurately."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.1, # 0.1 until 0.9\n",
        "    \"do_sample\": True, # not always the same answer\n",
        "}"
      ],
      "metadata": {
        "id": "iWhy80DmKuD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating output: output = pipe(messages, **generation_args): This line passes the input message and generation arguments to the text generation pipeline. The pipeline generates a response based on the input message and specified parameters.\n",
        "\n",
        "* `**generation_args`: This unpacks the generation_args dictionary and passes its contents as keyword arguments to the pipeline, customizing the text generation process."
      ],
      "metadata": {
        "id": "M7xsteKyHOzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is quantum computing?\"\n",
        "#prompt = \"Calculate 7 x 6 - 42?\"\n",
        "\n",
        "output = pipe(prompt, **generation_args)"
      ],
      "metadata": {
        "id": "9duBSdx9LhQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "9nxhZa_DMQBp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "9358ca27-50ce-4e73-f43e-751aad727ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'output' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-61353e8b2d5a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "JYpyXl3dMUnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Calculate 7 x 6 - 42?\"\n",
        "output = pipe(prompt, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "WNS53NtxModt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Let me know the first person in the moon?\"\n",
        "output = pipe(prompt, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "PAQi3iSENX6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the model continued generating after giving the answer, which is why this time it took longer.\n",
        "What happens is that the model continues \"talking to itself\", as if simulating a conversation. This is expected behavior since we do not define what we call an end token. This will be explained in detail, but for now what you need to know is that to avoid this behavior we use templates, which are generally recommended by the authors themselves (or by the community)\n",
        "\n",
        "\n",
        "\n",
        "And how to evaluate which model performs best for certain tasks?\n",
        "For example, here we now evaluate knowledge about facts in general, to find out which model performs best in this task you can search for benchmarks/tests and leaderboards"
      ],
      "metadata": {
        "id": "TdPtbBGDHhss"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI3Jt8AwRewE"
      },
      "source": [
        "## Templates and prompt engineering\n",
        "\n",
        "Prompt templates help translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand context and generate relevant, more coherent output."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Solve problem with text that continues to be generated after responding\n",
        "\n",
        "To find the appropriate template, always check the template description, for example: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
        "\n",
        "In the case of Phi 3, the authors recommend this template below.\n",
        "\n",
        "Note: later we will see a way to pull this manual template without having to manually copy and paste it here.\n",
        "\n",
        "These tags formed by `<|##nome##|>` are what we call special tokens and are used to delimit the beginning and end of text and tell the model how we want the message to be interpreted\n",
        "\n",
        "The special tokens used to interact with Phi 3 are these:\n",
        "\n",
        "* `<|system|>, <|user|> and <|assistant|> `: correspond to the role of the messages. The roles used here are: system, user and assistant\n",
        "\n",
        "* `<|end|>`: This is equivalent to the EOS (End of String) token, used to mark the end of the text/string.  \n",
        "\n",
        "We will use the .format to concatenate the prompt in this template, so we don't need to retype it manually"
      ],
      "metadata": {
        "id": "6y1YEYbBHpdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"<|system|>\n",
        "You are a helpful assistant.<|end|>\n",
        "<|user|>\n",
        "\"{}\"<|end|>\n",
        "<|assistant|>\"\"\".format(prompt)"
      ],
      "metadata": {
        "id": "CEm48xCkt3kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template"
      ],
      "metadata": {
        "id": "AysQM5LSuMLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(template, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "3iXWGH06uhDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this the problem was resolved. We can do another test (in fact, this prompt below can be used to easily test whether the model responds well in our language)"
      ],
      "metadata": {
        "id": "CBMSnk2aHu6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"VocÃª entende portuguÃªs?\"\n",
        "\n",
        "template = \"\"\"<|system|>\n",
        "You are a helpful assistant.<|end|>\n",
        "<|user|>\n",
        "\"{}\"<|end|>\n",
        "<|assistant|>\"\"\".format(prompt)\n",
        "\n",
        "output = pipe(template, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "CZ2Wi4GYu2fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is AI?\"  # @param {type:\"string\"}\n",
        "\n",
        "template = \"\"\"<|system|>\n",
        "You are a helpful assistant.<|end|>\n",
        "<|user|>\n",
        "\"{}\"<|end|>\n",
        "<|assistant|>\"\"\".format(prompt)\n",
        "\n",
        "output = pipe(template, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "-MafioFNvG4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this the problem was resolved. We can do another test (in fact, this prompt below can be used to easily test whether the model responds well in our language)"
      ],
      "metadata": {
        "id": "d5s4lwhELZKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"What is AI?\" # @param {type:\"string\"}\n",
        "\n",
        "sys_prompt = \"You are a helpful assistant.\"\n",
        "\n",
        "template = \"\"\"<|system|>\n",
        "{}<|end|>\n",
        "<|user|>\n",
        "\"{}\"<|end|>\n",
        "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
        "\n",
        "print(template)\n",
        "\n",
        "output = pipe(template, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "MiH0B6NRxQVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Example with code generation\n",
        "\n",
        "We can modify the system prompt further\n",
        "\n",
        "More modern models require less prompt engineering in this regard. Just saying it's an assistant is more than enough for most cases.\n"
      ],
      "metadata": {
        "id": "9v3jxQHZIQvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a python code to have the fibonnaci sequence\"\n",
        "\n",
        "sys_prompt = \"You are an expert programmer. Show that code and explain it\"\n",
        "\n",
        "template = \"\"\"<|system|>\n",
        "{}<|end|>\n",
        "<|user|>\n",
        "\"{}\"<|end|>\n",
        "<|assistant|>\"\"\".format(sys_prompt, prompt)\n",
        "\n",
        "output = pipe(template, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "3X__sE8LyQrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fibonacci(n):\n",
        "\n",
        "    a, b = 0, 1\n",
        "\n",
        "    sequence = []\n",
        "\n",
        "    while len(sequence) < n:\n",
        "\n",
        "        sequence.append(a)\n",
        "\n",
        "        a, b = b, a + b\n",
        "\n",
        "    return sequence\n",
        "\n",
        "\n",
        "# Exemplo de uso:\n",
        "\n",
        "n = 10  # Quantidade de nÃºmeros da sequÃªncia de Fibonacci a serem gerados\n",
        "\n",
        "print(fibonacci(n))"
      ],
      "metadata": {
        "id": "v4ynHmAeysiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improving results\n",
        "\n",
        "**Exploring changes to the prompt**\n",
        "* It may fail depending on the type of request. We will see many ways to improve the delivery of results.\n",
        "* But for now, remember to check if your prompt could not be more specific. If even after improving the prompt you are having difficulty achieving the expected result (and after trying other parameters), then the model is not so appropriate for this task.\n",
        "* > Bonus tip for code generation: An idea/suggestion for a prompt that could be convenient for code generation, in case you want to use LLM as your co-pilot:\n",
        "> \"Refactor using concepts such as SOLID, Clean Code, DRY, KISS and if possible apply one or more appropriate design patterns aiming at scalability and performance, creating an organized folder structure and separating by files\" (and of course, here you can modify as you wish)\n",
        "\n",
        "* Note: Sometimes it is better to keep the prompt simple and not too elaborate. Including too many different information or references can be confusing. Therefore, it is quite interesting to add or remove term by term if you are experimenting and looking for better results.\n",
        "\n",
        "**Exploring other models**\n",
        "* In this case, to achieve more assertive results you can look for larger and more modern models with more parameters (remember the trade-off between efficiency and quality of responses) or even models focused on the desired task, for example code generation or conversations/chat.\n",
        "* For the code generation example, you could use the [deepseek-coder 6.7B](https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct) model (or look for others with this focus)\n"
      ],
      "metadata": {
        "id": "zMD323zUIbdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Where to find prompts\n",
        "\n",
        "Creating your own prompt can be ideal if you want to reach very specific cases.\n",
        "But if you don't have much time to experiment (or don't know the best way) a good tip is to search for prompts on the internet.\n",
        "\n",
        "There are several websites and repositories that provide prompts made by the community.\n",
        "\n",
        "One example is the LangSmith hub: https://smith.langchain.com/hub It is part of the LangChain ecosystem. This will be very convenient later, as we will see how to pull prompts hosted there through just one function"
      ],
      "metadata": {
        "id": "x8-UKvePIivG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5z_t7TEiNOC"
      },
      "source": [
        "## Message Format\n",
        "\n",
        "An increasingly common use case for LLMs is chat. In a chat context, instead of continuing a single string of text (as is the case with a standard language model), the model continues a conversation consisting of one or more messages, each of which includes a role, such as \"user\" or \"assistant\", as well as message text.\n",
        "\n",
        "The prompt can therefore also be structured this way below. We will look at this in more detail when we are using LangChain, as we will have additional features that will improve the usability of this mode.\n",
        "\n",
        "`msg`: This list contains the input message that we want the model to respond to. The message format includes a dictionary with the keys `role` and `content`.\n",
        "\n",
        "* `role`: \"user\" indicates that the message is from the user. Other possible roles could include \"system\" or \"assistant\" if you are simulating a multi-turn conversation. Different models may have roles with different names, here with Phi 3 these are expected.\n",
        "\n",
        "* `content`: Here we leave the actual question we want the model to answer, in this case, our prompt.\n",
        "\n",
        "We will explore this mode further when we use LangChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"O que Ã© IA?\"\n",
        "\n",
        "msg = [\n",
        "    {\"role\": \"system\", \"content\": \"VocÃª Ã© um assistente virtual prestativo. Responda as perguntas em portuguÃªs.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "output = pipe(msg, **generation_args)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "LcR-WPYM2Bi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Liste o nome de 10 cidades famosas da Europa\"\n",
        "prompt_sys = \"VocÃª Ã© um assistente de viagens prestativo. Responda as perguntas em portuguÃªs.\"\n",
        "\n",
        "msg = [\n",
        "    {\"role\": \"system\", \"content\": prompt_sys},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "output = pipe(msg, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "jFJi-fUs28iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, when implementing more modern models in this way, such as Phi 3 or llama 3 for example, it is recommended to follow the appropriate template as mentioned above, which is an even more guaranteed way to prevent the model from hallucinating and continuing to talk to itself after giving the desired answer.\n",
        "\n",
        "This concern will not be necessary in some methods using LangChain, or using proprietary models such as Open AI and Gemini."
      ],
      "metadata": {
        "id": "lPdwp_xEJq8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Check GPU\n",
        "\n",
        "To check how many resources have already been consumed from the GPU, you can use the command below. When using these smaller models, we don't need to worry, but after very continuous use and after switching between several different models, it may be interesting to keep an eye on consumption, to avoid the error \"OutOfMemoryError: CUDA out of memory.\"\n",
        "\n",
        "If this error occurs, simply restart the session. Note: you need to re-run the import code and where the variables that will be used were declared."
      ],
      "metadata": {
        "id": "24mLEkWtJsyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "4lYDIN_gJwLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1hmnE3NLuIl"
      },
      "source": [
        "### Optimizing with quantization\n",
        "\n",
        "So far we have used a lightweight model (Phi-3 4k), but trying to run much larger models can be challenging given the limited resources, especially when using the free version of Google Colab. However, with quantization techniques and BitsAndBytesConfig from the transformers library, it is possible to load and run massive models efficiently, without significantly compromising performance.\n",
        "\n",
        "Quantization techniques reduce memory and computation costs by representing weights and activations with lower precision data types, such as 8-bit integers (int8). This allows us to load larger models and speed up inference.\n",
        "\n",
        "\n",
        "To run the model efficiently on Google Colab, we will use BitsAndBytesConfig to enable 4-bit quantization. This configuration helps reduce the memory footprint and computational load, making it feasible to use large models on limited hardware resources.\n",
        "\n",
        "More about quantization here:\n",
        "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "\n",
        "There are also other solutions for quantization (for example [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) or [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)) that may or may not optimize further.\n",
        "If you don't want to bother with optimization/performance and at the same time maintain quality then consider using a paid solution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "PFnMbJz76A-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the above parameters:\n",
        "\n",
        "`load_in_4bit` - This parameter enables 4-bit quantization. When set to True, model weights are loaded with 4-bit precision, significantly reducing memory usage.\n",
        "* Impact: Lower memory usage and faster calculations with minimal impact on model accuracy.\n",
        "\n",
        "`bnb_4bit_quant_type` - Specifies the type of 4-bit quantization to use. \"nf4\" stands for NormalFloat4, a quantization scheme that helps maintain model performance while reducing accuracy.\n",
        "* Impact: Balances the trade-off between model size and performance.\n",
        "\n",
        "`bnb_4bit_use_double_quant` - When set to True, this parameter enables double quantization, which further reduces quantization error and improves model stability.\n",
        "* Impact: Reduces quantization error, improving model stability.\n",
        "\n",
        "`bnb_4bit_compute_dtype` - defines the data type for calculations. Using torch.bfloat16 (Brain Floating Point) helps improve computational efficiency, maintaining most of the precision of 32-bit floating point numbers.\n",
        "* Impact: Efficient calculations with minimal loss of precision.\n",
        "\n",
        "To apply quantization, we will now load the model with the \"AutoModelForCausalLM\" method, as mentioned previously.\n",
        "\n",
        "**Note:** if you run without quantization, you will see that there will be a memory overflow problem in Colab. Therefore, it is now essential to use this technique if we are going to use it on Colab's free GPU"
      ],
      "metadata": {
        "id": "h3gZqfbvJ-h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "WOXF3tbwWIDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OYEeME9qQuAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, when preparing data for LLM, one uses tokenizer.apply_chat_template, which adds EOS (end of sequence) tokens after each response.\n",
        "\n",
        "More on templates in chat templates\n",
        "https://huggingface.co/docs/transformers/chat_templating"
      ],
      "metadata": {
        "id": "8IrOEwelKFXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\"Quem foi a primeira pessoa no espaÃ§o?\")\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]"
      ],
      "metadata": {
        "id": "f8wA32QtYMIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We recommend using the Hugging Face tokenizer.apply_chat_template() function, which automatically applies the correct chat template to the respective model. It is easier than manually writing the chat template and less error-prone. `return_tensors=\"en\"` specifies that the returned tensors should be in PyTorch format.\n",
        "\n",
        "The remaining lines of code: tokenize the input messages, move the tensors to the correct device, generate new tokens based on the given inputs, decode the generated tokens back into readable text, and finally return the generated text.\n",
        "\n",
        "* `model_inputs = encodeds.to(device)` - Moves the encoded tensors to the specified device (CPU or GPU) for processing by the model.\n",
        "\n",
        "* `encodeds` - The tensors generated in the previous line.\n",
        "\n",
        "`to(device)` - Moves the tensors to the specified device (device), which can be a CPU or GPU.\n",
        "\n",
        "* `generated_ids = model.generate...` -> Generates a sequence of tokens from the model_inputs.\n",
        "* model.generate: Model function that generates text based on the provided inputs.\n",
        "* model_inputs: The processed inputs, ready to be used by the model.\n",
        "* max_new_tokens=1000: Limits generation to a maximum of 1000 new tokens.\n",
        "* do_sample=True: Enables random sampling during generation, which can result in more varied outputs.\n",
        "*\n",
        "\n",
        "pad_token_id=tokenizer.eos_token_id: Sets the padding token to be the end-of-sequence token, ensuring that the generation is properly terminated.\n",
        "\n",
        "* `decoded = tokenizer.batch_decode(generated_ids)` - decodes the generated IDs back to readable text.\n",
        "* `tokenizer.batch_decode` - a function that converts a list of token IDs back to text.\n",
        "* `generated_ids` - the IDs of the tokens generated in the previous step.\n",
        "\n",
        "`res = decoded[0]` - extracts the first item from the decoded text list.\n",
        "decoded[0]: Gets the first text from the decoded list, which corresponds to the generated text for the first (and possibly only) input provided."
      ],
      "metadata": {
        "id": "YkMxyMUxKMuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "model_inputs = encodeds.to(device)\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens = 1000, do_sample = True,\n",
        "                               pad_token_id=tokenizer.eos_token_id)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "res = decoded[0]\n",
        "res"
      ],
      "metadata": {
        "id": "DK9DTtK0YZdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "****\n",
        "You will see that with LangChain, we will have more options and tools, as the library offers a complete ecosystem integrated with the main and most modern language modeling solutions, both open and private.\n",
        "\n",
        "So why might it be interesting to know this method that we show now, if LangChain is better and offers more options? It can be useful if you are testing a new and recently published model that does not yet have much compatibility.\n",
        "\n",
        "Even with LangChain, when dealing with literally thousands of different models, there may be some incompatibility when loading them. This is usually fixed by the development team in some future release, but it is not always immediate - and other solutions you will find only by searching on forums since they are published by the community.\n",
        "\n",
        "So knowing this method can be useful if you are testing the latest open-source models that did not load correctly with LangChain.\n",
        "\n",
        "It may be a small inconvenience for some, but it is necessary to understand that this is the \"price\" to pay for being on the frontier and using the most modern Open Source models and being able to use them for free."
      ],
      "metadata": {
        "id": "7SaVFxREKVpH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}